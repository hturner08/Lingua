# Lingua 
## Background 
Automatic speech recognition(ASR), a subfield of computer science and computational linguistics, is concerned with developing methodologies and technologies that enable the recognition and translation of spoken languages into text using computers. The first major break-through came in the 1970s, when researchers began using Hidden Markov Models(HMM), statistical models for modeling systems, to characterize the temporal and spectral properties of speech. HMMs are still an industry standard for speech recognition models, but their size means they must often be hosted in the cloud and require an internet connection to use. More recently, with the explosion of deep learning, artificial neural networks have also been employed in various ASR models as well as automated lip reading(ALR) models. 
Humans, especially deaf/HOH people, have never recognized speech purely auditorily. Rather, we use a combination of audio, gestures, lip reading, and other context clues. Focusing specifically on audio and lip reading, each provides us with a different benefit. While audio is generally most clear in vowel sounds and plosive consonants, lip reading can distinguish between softer consonants, such as fricatives, nasals, laterals, and affricates. By implementing an ASR model that takes both auditory and visual input, we can more closely model how we actually recognize speech in machine learning models, and hopefully achieve greater success.

## Project Overview
In our project, we intend to implement an ALR/ASR model that takes the benefits of both lip-reading image recognition models and audio-based speech recognition models and combine them. This will include analyzing both ALR and ASR only models, and specifically determining their strong points (hypothesized to be vowels for ASR, consonants for ALR), and then combining the two models to amplify the strong suits of each. We will also translate the models to work in real-time, rather than to create the transcript after the conversation. In this case, we will have to rely less on the overall context of the conversation and more on combining audio and visual cues, word by word. Through the use of AR glasses, we plan to dynamically overlay the live transcription next to the speaker. In our project we will utilize the glassesâ€™ spatial audio capabilities alongside our speech recognition model to implement a feature to distinguish between speakers, which will be denoted by different colored texts. Opposed to traditional speech to text applications on smartphones, creating this application in AR allows for eye contact and a more natural conversation, unhindered by a screen. Additionally, in the case where there are multiple speakers, spatially placing the text makes it easy to tell what each person is saying. This is an advantage of making this project in AR, because in similar smartphone applications, the audio from multiple people gets compiled into one jumbled transcript, making it difficult to make out what each individual person said.

## Current Tasks
- Investigate various audio and visual datasets for speech recognition
- Write shell script to generate our dataset
- Test out different models in jupyter notebooks